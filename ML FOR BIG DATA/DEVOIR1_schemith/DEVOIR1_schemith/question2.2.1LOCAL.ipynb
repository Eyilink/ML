{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[8]\")\\\n",
    "        .appName(\"Abdenour\")\\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count lines :::  300\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "# Load and parse the data file into an RDD of LabeledPoint.\n",
    "data = MLUtils.loadLibSVMFile(sc, 'iris_libsvm.txt')\n",
    "data_merged = data\n",
    "for i in range(1):\n",
    "    data_merged = data_merged.union(data)\n",
    "\n",
    "data = data_merged\n",
    "print(\"Count lines ::: \", data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(0.0, (4,[0,1,2,3],[5.1,3.5,1.4,0.2]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "(trainingData, testData) = data.randomSplit([1.0, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 12.481865644454956 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},impurity='entropy', maxDepth=5, maxBins=32)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeModel classifier of depth 5 with 17 nodes\n",
      "  If (feature 2 <= 2.45)\n",
      "   Predict: 0.0\n",
      "  Else (feature 2 > 2.45)\n",
      "   If (feature 3 <= 1.75)\n",
      "    If (feature 2 <= 4.95)\n",
      "     If (feature 3 <= 1.65)\n",
      "      Predict: 1.0\n",
      "     Else (feature 3 > 1.65)\n",
      "      Predict: 2.0\n",
      "    Else (feature 2 > 4.95)\n",
      "     If (feature 3 <= 1.55)\n",
      "      Predict: 2.0\n",
      "     Else (feature 3 > 1.55)\n",
      "      If (feature 0 <= 6.75)\n",
      "       Predict: 1.0\n",
      "      Else (feature 0 > 6.75)\n",
      "       Predict: 2.0\n",
      "   Else (feature 3 > 1.75)\n",
      "    If (feature 2 <= 4.85)\n",
      "     If (feature 0 <= 5.95)\n",
      "      Predict: 1.0\n",
      "     Else (feature 0 > 5.95)\n",
      "      Predict: 2.0\n",
      "    Else (feature 2 > 4.85)\n",
      "     Predict: 2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.toDebugString())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.16666666666666666\n",
      "Learned classification tree model:\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[2]\")\\\n",
    "        .appName(\"Abdenour\")\\\n",
    "        .getOrCreate()\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "data = MLUtils.loadLibSVMFile(sc, 'iris_libsvm.txt')\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "for i in range(2):\n",
    "    trainingData = trainingData.union(trainingData)\n",
    "    testData = testData.union(testData)\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},impurity='entropy', maxDepth=5, maxBins=32)\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.04081632653061224\n",
      "Learned classification tree model:\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[4]\")\\\n",
    "        .appName(\"Abdenour\")\\\n",
    "        .getOrCreate()\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "data = MLUtils.loadLibSVMFile(sc, 'iris_libsvm.txt')\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "for i in range(2):\n",
    "    trainingData = trainingData.union(trainingData)\n",
    "    testData = testData.union(testData)\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},impurity='entropy', maxDepth=5, maxBins=32)\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.07692307692307693\n",
      "Learned classification tree model:\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[6]\")\\\n",
    "        .appName(\"Abdenour\")\\\n",
    "        .getOrCreate()\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "data = MLUtils.loadLibSVMFile(sc, 'iris_libsvm.txt')\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "for i in range(2):\n",
    "    trainingData = trainingData.union(trainingData)\n",
    "    testData = testData.union(testData)\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},impurity='entropy', maxDepth=5, maxBins=32)\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.023255813953488372\n",
      "Learned classification tree model:\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[8]\")\\\n",
    "        .appName(\"Abdenour\")\\\n",
    "        .getOrCreate()\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "data = MLUtils.loadLibSVMFile(sc, 'iris_libsvm.txt')\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "for i in range(2):\n",
    "    trainingData = trainingData.union(trainingData)\n",
    "    testData = testData.union(testData)\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},impurity='entropy', maxDepth=5, maxBins=32)\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.05128205128205128\n",
      "Learned classification tree model:\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    sc.stop()\n",
    "except:\n",
    "    print()\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[10]\")\\\n",
    "        .appName(\"Abdenour\")\\\n",
    "        .getOrCreate()\n",
    "from pyspark import SparkContext\n",
    "sc = spark.sparkContext.getOrCreate()\n",
    "data = MLUtils.loadLibSVMFile(sc, 'iris_libsvm.txt')\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
    "for i in range(2):\n",
    "    trainingData = trainingData.union(trainingData)\n",
    "    testData = testData.union(testData)\n",
    "\n",
    "model = DecisionTree.trainClassifier(trainingData, numClasses=3, categoricalFeaturesInfo={},impurity='entropy', maxDepth=5, maxBins=32)\n",
    "predictions = model.predict(testData.map(lambda x: x.features))\n",
    "labelsAndPredictions = testData.map(lambda lp: lp.label).zip(predictions)\n",
    "testErr = labelsAndPredictions.filter(\n",
    "    lambda lp: lp[0] != lp[1]).count() / float(testData.count())\n",
    "print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
